{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Spam_Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Estrada-John/SpamClassification/blob/master/Spam_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "87b850d02f11097c58dd2fcadb9473490f0fbec5",
        "id": "8CPjBm9Va7mf",
        "colab_type": "text"
      },
      "source": [
        "<h3><center>ECE 49500/59500 Machine Learning<center>\n",
        "<center>Spring 2020<center>\n",
        "<h2><center>Spam Email Classification using Naive Bayes Classifier<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5f5829dce6459c732b1989fc648c2487c08136ed",
        "id": "JDoiqmm9a7mj",
        "colab_type": "text"
      },
      "source": [
        "## Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "884df48ea2c459c206c0afa5c375b07b7a320481",
        "id": "7EmL7D5Za7mk",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, you will use the SMS Spam Collection dataset which is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "06a0e1534b0a88594e3df8d4a5ffbea4b0379dad",
        "id": "kTSLC8pna7mk",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3e73380ed292d21384e3409f8b7793f03bce7ffe",
        "id": "ympv-3IPa7mm",
        "colab_type": "text"
      },
      "source": [
        "The files contain one message per line. Each line is composed by two columns: **v1 contains the label (ham or spam) and v2 contains the raw text.**  This corpus has been collected from free or free for research sources at the Internet. More details can be found in here: https://www.kaggle.com/uciml/sms-spam-collection-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrjcagIpa7mn",
        "colab_type": "text"
      },
      "source": [
        "## Objective\n",
        "\n",
        "Apply Naive Bayes Classifier to this dataset to accurately predict which texts are spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8a5a3441f25a0f7a9c14196369621a87e99327cc",
        "id": "0nk3BhJGa7mo",
        "colab_type": "text"
      },
      "source": [
        "## 1. Contents of this notebook\n",
        "\n",
        "*  Text Analysis\n",
        "        - Explore the Data\n",
        "        - Developing Insights\n",
        "*  Test Transformation\n",
        "        - Data Cleaning (Removing unimportant data/ Stopwords/ Stemming)\n",
        "        - Converting data into a model usable format (Bag of words Model)\n",
        "*  Naive Bayes Model for Spam Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c556276e9998629e9d3f9e68251ca4023b6bf552",
        "id": "kGOW3zlpa7mo",
        "colab_type": "text"
      },
      "source": [
        "#### TEXT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "54f4d31239a0647338b76c7d35f9eb115c8ab383",
        "id": "-PyMOKtCa7mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3859f1dd-69f0-46f5-e920-e4fc51f88e54"
      },
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text Preprocessing\n",
        "import nltk\n",
        "nltk.download(\"all\")   # you will need to download it if you have not done so\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz_b2L5Xa7mu",
        "colab_type": "text"
      },
      "source": [
        "* #####  Load dataset.  We will use Pandas library to load the dataset. More information regarding Pandas can be found at https://pandas.pydata.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9ed1ededa351f22ba7d8e53c405cc67ebb5dfb8c",
        "id": "jhkEAjF_a7mu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "9d82fd26-5790-47e6-a430-547fb3b4bb69"
      },
      "source": [
        "messages = pd.read_csv(\"spam.csv\", encoding = 'latin-1')\n",
        "\n",
        "# Drop the extra columns and rename columns\n",
        "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
        "messages.columns = [\"category\", \"text\"]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5b4fc0fa5431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spam.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Drop the extra columns and rename columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Unnamed: 2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unnamed: 3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unnamed: 4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'spam.csv' does not exist: b'spam.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "42bdc935918fe49044e615c0dfbac06e642da12a",
        "id": "6NMPp7Lca7mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(messages.head(n = 20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmlmfhT_a7m1",
        "colab_type": "text"
      },
      "source": [
        "* ##### Check overall information of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b85a7790c16cb92c0dc5af97389e8e745a76eef3",
        "id": "TUBchp_oa7m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4ce10b8024721b3c97a13f572a78145069f1ae6c",
        "collapsed": true,
        "id": "wdrKxZBXa7m6",
        "colab_type": "text"
      },
      "source": [
        "* ##### Let us see what precentage of our data is spam or ham (legitimate)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3c38ff46dfb45305e6bcb4ef9613d35fb81e6dd9",
        "id": "KQlk_zTma7m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages[\"category\"].value_counts().plot(kind = 'pie', figsize = (6, 6), fontsize=14, autopct = '%1.1f%%', shadow = True)\n",
        "plt.ylabel(\"Spam vs Ham\")\n",
        "plt.legend([\"Ham\", \"Spam\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "463c4b88d40f44868714a92a6bc82e155f72e8a7",
        "id": "Y0hatVcEa7m9",
        "colab_type": "text"
      },
      "source": [
        "From above Pie chart, it can be seen that about 86% of our dataset consists of non-spam messages. \n",
        "\n",
        "*  As we split our data set into train and test, **stratified sampling** is recommended in this case, otherwise we have a chance of our training model being skewed towards normal messages. If the sample we choose to train our model consists majorly of normal messages, it may end up predicting everything as ham and we might not be able to figure this out since most of the messages we get are actually ham and will have a pretty good accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lZVmW06a7m-",
        "colab_type": "text"
      },
      "source": [
        "* #####  now let us check individual Spam/ham words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7107711acdddde2d5d974141563d61ad017a9632",
        "id": "NWbtrISta7m_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_messages = messages[messages[\"category\"] == \"spam\"][\"text\"]\n",
        "ham_messages = messages[messages[\"category\"] == \"ham\"][\"text\"]\n",
        "\n",
        "spam_words = []\n",
        "ham_words = []\n",
        "\n",
        "# Since this is just classifying the message as spam or ham, we can use isalpha(). \n",
        "# This will also remove the not word in something like can't etc. \n",
        "# In a sentiment analysis setting, it's better to use sentence.translate(string.maketrans(\"\", \"\", ), chars_to_remove)\n",
        "\n",
        "def extractSpamWords(spamMessages):\n",
        "    global spam_words\n",
        "    words = [word.lower() for word in word_tokenize(spamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
        "    spam_words = spam_words + words\n",
        "    \n",
        "def extractHamWords(hamMessages):\n",
        "    global ham_words\n",
        "    words = [word.lower() for word in word_tokenize(hamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
        "    ham_words = ham_words + words\n",
        "\n",
        "spam_messages.apply(extractSpamWords)\n",
        "ham_messages.apply(extractHamWords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3A2CQk4a7nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total Messages:\" , len(ham_messages) + len(spam_messages))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "74025e5ea690679aef56752c7e8ecfcf9e9816f2",
        "id": "BkIFuUbMa7nF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top 10 spam words\n",
        "spam_words = np.array(spam_words)\n",
        "print(\"Top 10 Spam words are :\\n\")\n",
        "pd.Series(spam_words).value_counts().head(n = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f17de5246259a3b3978aeafeb9d5d2af743b6264",
        "id": "79Ws2EHQa7nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top 10 Ham words\n",
        "ham_words = np.array(ham_words)\n",
        "print(\"Top 10 Ham words are :\\n\")\n",
        "pd.Series(ham_words).value_counts().head(n = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cacf007cf2d004b2c51273d31f5e370d3945e373",
        "collapsed": true,
        "id": "coVWhhTDa7nK",
        "colab_type": "text"
      },
      "source": [
        "* #### Does the length of the message indicates us anything?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c0b15b50fe828f9af19f5a5287f33bdab5af71f1",
        "id": "bWLGDaAsa7nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages[\"messageLength\"] = messages[\"text\"].apply(len)\n",
        "messages[\"messageLength\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "89f11851732828c5fd43d91aa5feffabafe6f749",
        "id": "y6ECNyJWa7nO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, ax = plt.subplots(2, 1, figsize = (6, 10))\n",
        "\n",
        "sns.distplot(messages[messages[\"category\"] == \"spam\"][\"messageLength\"], bins = 20, ax = ax[0])\n",
        "ax[0].set_xlabel(\"Spam Message Word Length\")\n",
        "\n",
        "sns.distplot(messages[messages[\"category\"] == \"ham\"][\"messageLength\"], bins = 20, ax = ax[1])\n",
        "ax[1].set_xlabel(\"Ham Message Word Length\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1ef76f1e0a1ee2cb5087f8535ca7f9d03a960a56",
        "id": "QdHBZwoBa7nS",
        "colab_type": "text"
      },
      "source": [
        "**It can be observed that spam messages are usually longer which could be a a feature to predict whether the message is spam/ ham. Right?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c4bb350aff1e1e5641c133c43477e5bc08826226",
        "collapsed": true,
        "id": "R98ik1hQa7nT",
        "colab_type": "text"
      },
      "source": [
        "#### TEXT TRANSFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2b96c635ef437a18a2e20fc4aea0d8327e1c1bc3",
        "id": "9Jjm2l0Ia7nT",
        "colab_type": "text"
      },
      "source": [
        "#### Lets clean our data by removing punctuations/ stopwords and stemming words\n",
        "* __Stemming__ reduces related words to a common stem. e.g., fish and fishes become 'fish'\n",
        "* __Stop words__ are commonly used words that are unlikely to have any benefit in natural language processing. These includes words such as ‘a’, ‘the’, ‘is’.\n",
        "\n",
        "More references: https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6df4f9e7df701b1eee1e2adf212fe457f09e9159",
        "id": "6HKl9aN3a7nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "def stemmer(text):\n",
        "    text = text.split()\n",
        "    words = \"\"\n",
        "    for i in text:\n",
        "            stemmer = SnowballStemmer(\"english\")\n",
        "            words += (stemmer.stem(i))+\" \"\n",
        "    return words\n",
        "\n",
        "def puncStopW(text):\n",
        "    \n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    return \" \".join(text)\n",
        "\n",
        "messages[\"text\"] = messages[\"text\"].apply(stemmer)\n",
        "messages[\"text\"] = messages[\"text\"].apply(puncStopW)\n",
        "messages.head(n = 10)     # You may compare the different between orginaltext and filtered one to see the difference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dURYI5D-a7nX",
        "colab_type": "text"
      },
      "source": [
        "##### Convert the clean text into a feature representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7rhZhEXa7nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "features_np = vec.fit_transform(messages[\"text\"]).toarray()  # converting to array\n",
        "print(features_np.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "55093358b57afaa87db05552948518d4ef206c8f",
        "id": "RW21GQWxa7nZ",
        "colab_type": "text"
      },
      "source": [
        "## 2. MODEL APPLICATION\n",
        "\n",
        "In this section, you will implement the Naive Bayes Classifier to the input data and predict a given email is spam or ham. \n",
        "\n",
        "### 2.1 Firstly, convert category of SPAM and HAM messages into 1 and 0, respectively. And then split the data into training set and test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "250ef549d64b975b0723a42ef3bad50e809daca9",
        "id": "piSMWUowa7na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(messages[\"category\"])\n",
        "def encodeCategory(cat):\n",
        "    if cat == \"spam\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "       \n",
        "messages[\"category\"] = messages[\"category\"].apply(encodeCategory)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "messages_np = messages[\"category\"].to_numpy()\n",
        "print(messages_np)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb7S4Hpma7nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_np, messages_np, stratify = messages_np, test_size = 0.3, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmLFDJKga7ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The size of ham messages:\", len(ham_messages))\n",
        "print(\"The size of spam messages:\", len(spam_messages))\n",
        "print(\"The size of total samples:\", len(messages[\"category\"]))\n",
        "print(\"The size of trainng samples:\", X_train.shape)\n",
        "print(\"The size of testing samples:\", X_test.shape)\n",
        "print(\"The size of spam messages in training samples:\", len(y_train[y_train==1]))\n",
        "print(\"The size of ham messages in training samples:\", len(y_train[y_train==0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6b41983d5b4f3ac006f3a001d4f45b156fb2452d",
        "id": "SZ-MSpbMa7nj",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Naive Bayes Classifier Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqnUI7zla7nj",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Sort data into two classes: spam and non-spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A88cZiTMa7nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xy0 = X_train[y_train == 0]\n",
        "Xy1 = X_train[y_train == 1]\n",
        "print(\"The size of ham samples in training data:\", Xy0.shape)\n",
        "print(\"The size of spam samples in training data:\", Xy1.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dos5eLUua7nm",
        "colab_type": "text"
      },
      "source": [
        "#### 2) Calculate conditional Probability, prior probability (Feel free to build functions and use multiple cells to complete this step)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpKiawZAa7nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1)\n",
        "priorHam = Xy0.shape[0] / X_train.shape[0]\n",
        "priorSpam = Xy1.shape[0] / X_train.shape[0]\n",
        "\n",
        "#\n",
        "totalRowHam = np.sum(Xy0, axis=1)\n",
        "wordsHam = np.sum(totalRowHam, axis=0) \n",
        "\n",
        "totalRowSpam = np.sum(Xy1, axis=1)\n",
        "wordsSpam = np.sum(totalRowSpam, axis=0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Khn2apa7np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.0) probability of each word given a specific class\n",
        "def conditional(Spam, Ham):\n",
        "    ProbSpam = np.sum(Xy1, axis=0)\n",
        "    ProbHam = np.sum(Xy0, axis=0)\n",
        "    probabilityS = lambda x: (x + 1) / (wordsSpam + X_train.shape[1])\n",
        "    probabilityH = lambda x: (x + 1) / (wordsHam + X_train.shape[1])\n",
        "    return probabilityS(ProbSpam), probabilityH(ProbHam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF7YB5cPa7nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.1) get ptobability\n",
        "ConditionalSpam, ConditionalHam = conditional(Xy1, Xy0)\n",
        "print(ConditionalHam)\n",
        "print(ConditionalSpam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x674HqZ1a7nt",
        "colab_type": "text"
      },
      "source": [
        "#### 3) Classify test examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC5RGy2Ma7nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posteProb_spam = np.array([])\n",
        "posteProb_ham = np.array([])\n",
        "X_testTemp = np.array([])\n",
        "prob_s = 1\n",
        "prob_h = 1\n",
        "for i in range(0, X_test.shape[0]):\n",
        "    for j in range(0, X_test.shape[1]):\n",
        "        if(X_test[i][j] == 1): \n",
        "            prob_s *= ConditionalSpam[j]\n",
        "            prob_h *= ConditionalHam[j]\n",
        "    posteProb_spam = np.append(posteProb_spam, prob_s * priorSpam)\n",
        "    posteProb_ham = np.append(posteProb_ham, prob_h * priorHam )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ46ABija7nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = lambda x,y : x*y if (x == 1).all else (x*1)\n",
        "a = converter(X_train, ConditionalSpam)\n",
        "b = converter(X_train, ConditionalHam)\n",
        "w1 = np.array([])\n",
        "w2 = np.array([])\n",
        "for i in range(0, X_test.shape[0]):\n",
        "    z = np.trim_zeros(a[i])\n",
        "    y = np.trim_zeros(b[i])\n",
        "    w1 = np.append(w1, np.prod(z))\n",
        "    w2 = np.append(w2, np.prod(y))\n",
        "    #print(w1, w2)\n",
        "\n",
        "preAcc = lambda x,y : 1 if (x > y).any else (0)\n",
        "#zx = preAcc(w1, w2)\n",
        "print(w1)\n",
        "#print(np.trim_zeros(a.))\n",
        "#np.trim_zeros(a[0])\n",
        "#np.multiply(a[0])\n",
        "#np.count_nonzero(a[0] > 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_068z2WGa7nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totalRowHam = np.sum(X_test, axis=1)\n",
        "totalRowHam[0:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxYBqAmIa7n1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = np.array([])\n",
        "for i in range(0, X_test.shape[0]):\n",
        "    if (prob_s > prob_h):\n",
        "        temp = np.append(temp, 1)\n",
        "    else:\n",
        "        temp = np.append(temp, 0)\n",
        "\n",
        "accuracy = 0\n",
        "for i in range(0, X_test.shape[0]):\n",
        "    if (temp[i] == y_test[i]):\n",
        "        accuracy += 1 \n",
        "        \n",
        "print(accuracy / y_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEZ4qU7a7n3",
        "colab_type": "text"
      },
      "source": [
        "#### 4) Compute prediction accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWI5aMyda7n6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J17TBV0ra7n8",
        "colab_type": "text"
      },
      "source": [
        "<h2><center>Enjoy !<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5riVS6Qa7n9",
        "colab_type": "text"
      },
      "source": [
        "### <center>Acknowledgement<center>\n",
        "**The section 1** in this exercise is modified from online source: https://www.kaggle.com/ishansoni/sms-spam-collection-dataset\n",
        "\n",
        "The original dataset can be found here (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). More references can be found at http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ and A comprehensive study of this corpus in the following paper. \n",
        "\n",
        "Almeida, T.A., GÃ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011."
      ]
    }
  ]
}